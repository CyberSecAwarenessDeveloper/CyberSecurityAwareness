# -*- coding: utf-8 -*-
"""Visual_Cue_in_Phishing_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12jM8kM6mJlD1PzdHSNEISGi8ztWQyij7
"""

#!pip install pandas openpyxl

import pandas as pd

# Load the Excel file
file_path = '/path/to/Combined_ready.xlsx'  # replace with your file name
excel_data = pd.ExcelFile(file_path)

# List all sheet names to see the available sheets
excel_data.sheet_names

# Load specific sheets into separate DataFrames
sheet1 = pd.read_excel(excel_data, sheet_name='Form Responses 1')
sheet2 = pd.read_excel(excel_data, sheet_name='Form Responses 2')
sheet3 = pd.read_excel(excel_data, sheet_name='Form Responses 3')
sheet4 = pd.read_excel(excel_data, sheet_name='Form Responses 4')
sheet5 = pd.read_excel(excel_data, sheet_name='Form Responses 5')

# Check the unique answers in the 'Email-1' column
email_1_answers = sheet2['Email-1'].value_counts()

# Display the result
print(email_1_answers)

import pandas as pd

# Assuming Email-1 is Phishing, the true label for all responses is 'Phishing'
true_labels = ['Phishing'] * len(sheet2)

# Ensure that participant responses are stripped of any leading/trailing spaces and match the correct case
participant_responses = sheet2['Email-1'].str.strip().str.capitalize()  # Clean up the responses

# Map "Non-phishing" to "Legit" to standardize labels
participant_responses = participant_responses.replace('Non-phishing', 'Legit')

# Display the unique values of responses to ensure they are correct
#print(participant_responses.unique())

# Convert both true labels and participant responses to pandas Series for proper comparison
true_labels_series = pd.Series(true_labels)
participant_responses_series = pd.Series(participant_responses)

# Calculate True Positive (TP), False Positive (FP), True Negative (TN), False Negative (FN)
TP = ((true_labels_series == 'Phishing') & (participant_responses_series == 'Phishing')).sum()
FP = ((true_labels_series == 'Legit') & (participant_responses_series == 'Phishing')).sum()
TN = ((true_labels_series == 'Legit') & (participant_responses_series == 'Legit')).sum()
FN = ((true_labels_series == 'Phishing') & (participant_responses_series == 'Legit')).sum()

# Calculate Accuracy, Precision, Recall, and F1 Score
accuracy = (TP + TN) / len(true_labels)  # Total correct predictions / total samples
precision = TP / (TP + FP) if (TP + FP) != 0 else 0  # Avoid division by zero
recall = TP / (TP + FN) if (TP + FN) != 0 else 0  # Avoid division by zero
f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0  # Avoid division by zero

# Display the results
print(f'Accuracy: {accuracy * 100:.2f}%')
print(f'Precision: {precision * 100:.2f}%')
print(f'Recall: {recall * 100:.2f}%')
print(f'F1 Score: {f1_score * 100:.2f}%')

# Check the unique answers in the 'Email-1' column
email_1_answers = sheet2['Email-2'].value_counts()

# Display the result
print(email_1_answers)

import pandas as pd

# True label for Email-2 (based on your given data)
true_label_email_2 = 'Non-phishing'

# Get participant responses for Email-2 (assuming you have this data in 'sheet2')
participant_responses_email_2 = sheet2['Email-2'].str.strip().str.capitalize()  # Clean up responses

# Display the unique responses for Email-2
email_2_answers = participant_responses_email_2.value_counts()
print("Participant Responses for Email-2:")
print(email_2_answers)

# Convert to pandas Series for proper comparison
true_labels_series_email_2 = [true_label_email_2] * len(participant_responses_email_2)
true_labels_series_email_2 = pd.Series(true_labels_series_email_2)
participant_responses_series_email_2 = pd.Series(participant_responses_email_2)

# Calculate True Positive (TP), False Positive (FP), True Negative (TN), False Negative (FN) for Email-2
TP_email_2 = ((true_labels_series_email_2 == true_label_email_2) & (participant_responses_series_email_2 == true_label_email_2)).sum()
FP_email_2 = ((true_labels_series_email_2 != true_label_email_2) & (participant_responses_series_email_2 == true_label_email_2)).sum()
TN_email_2 = ((true_labels_series_email_2 != true_label_email_2) & (participant_responses_series_email_2 != true_label_email_2)).sum()
FN_email_2 = ((true_labels_series_email_2 == true_label_email_2) & (participant_responses_series_email_2 != true_label_email_2)).sum()

# Calculate Accuracy, Precision, Recall, and F1 Score for Email-2
accuracy_email_2 = (TP_email_2 + TN_email_2) / len(true_labels_series_email_2)  # Total correct predictions / total samples
precision_email_2 = TP_email_2 / (TP_email_2 + FP_email_2) if (TP_email_2 + FP_email_2) != 0 else 0  # Avoid division by zero
recall_email_2 = TP_email_2 / (TP_email_2 + FN_email_2) if (TP_email_2 + FN_email_2) != 0 else 0  # Avoid division by zero
f1_score_email_2 = 2 * (precision_email_2 * recall_email_2) / (precision_email_2 + recall_email_2) if (precision_email_2 + recall_email_2) != 0 else 0  # Avoid division by zero

# Display the results for Email-2
print(f'\nPerformance Metrics for Email-2:')
print(f'  Accuracy: {accuracy_email_2 * 100:.2f}%')
print(f'  Precision: {precision_email_2 * 100:.2f}%')
print(f'  Recall: {recall_email_2 * 100:.2f}%')
print(f'  F1 Score: {f1_score_email_2 * 100:.2f}%')

import pandas as pd

# Example: Load the sheet2 DataFrame from an Excel file
sheet_data = sheet2

# True labels for the 10 emails (based on your given data)
true_labels_dict = {
    'Email-1': 'Phishing',
    'Email-2': 'Non-phishing',
    'Email-3': 'Non-phishing',
    'Email-4': 'Phishing',
    'Email-5': 'Phishing',
    'Email-6': 'Non-phishing',
    'Email-7': 'Phishing',
    'Email-8': 'Non-phishing',
    'Email-9': 'Phishing',
    'Email-10': 'Non-phishing'
}

# Iterate over all emails and calculate performance metrics
for i in range(1, 11):
    # Define the current email column and true label
    email_column = f'Email-{i}'
    true_label = true_labels_dict[email_column]

    # Get participant responses for this email column (assumed to be in sheet_data)
    participant_responses = sheet_data[email_column].str.strip().str.capitalize()  # Clean up responses

    # Convert to pandas Series for proper comparison
    true_labels_series = [true_label] * len(participant_responses)
    true_labels_series = pd.Series(true_labels_series)
    participant_responses_series = pd.Series(participant_responses)

    # Calculate True Positive (TP), False Positive (FP), True Negative (TN), False Negative (FN)
    TP = ((true_labels_series == true_label) & (participant_responses_series == true_label)).sum()
    FP = ((true_labels_series != true_label) & (participant_responses_series == true_label)).sum()
    TN = ((true_labels_series != true_label) & (participant_responses_series != true_label)).sum()
    FN = ((true_labels_series == true_label) & (participant_responses_series != true_label)).sum()

    # Calculate Accuracy, Precision, Recall, and F1 Score
    accuracy = (TP + TN) / len(true_labels_series)  # Total correct predictions / total samples
    precision = TP / (TP + FP) if (TP + FP) != 0 else 0  # Avoid division by zero
    recall = TP / (TP + FN) if (TP + FN) != 0 else 0  # Avoid division by zero
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0  # Avoid division by zero

    # Display the results for each email
    print(f'Results for {email_column} from sheet_data:')
    print(f'  Accuracy: {accuracy * 100:.2f}%')
    print(f'  Precision: {precision * 100:.2f}%')
    print(f'  Recall: {recall * 100:.2f}%')
    print('---')

import pandas as pd

sheet_data = sheet2
# True labels for the 10 emails (based on your given data)
true_labels_dict = {
    'Email-1': 'Phishing',
    'Email-2': 'Non-phishing',
    'Email-3': 'Non-phishing',
    'Email-4': 'Phishing',
    'Email-5': 'Phishing',
    'Email-6': 'Non-phishing',
    'Email-7': 'Phishing',
    'Email-8': 'Non-phishing',
    'Email-9': 'Phishing',
    'Email-10': 'Non-phishing'
}


# Initialize a list to store the accuracy for each email
accuracy_list = []

# Iterate over all emails and calculate individual accuracy
for i in range(1, 11):
    email_column = f'Email-{i}'
    true_label = true_labels_dict[email_column]

    # Get participant responses for this email column (assumed to be in sheet_data)
    participant_responses = sheet_data[email_column].str.strip().str.capitalize()  # Clean up responses

    # Convert to pandas Series for proper comparison
    true_labels_series = [true_label] * len(participant_responses)
    true_labels_series = pd.Series(true_labels_series)
    participant_responses_series = pd.Series(participant_responses)

    # Calculate True Positive (TP), False Positive (FP), True Negative (TN), False Negative (FN)
    TP = ((true_labels_series == true_label) & (participant_responses_series == true_label)).sum()
    FP = ((true_labels_series != true_label) & (participant_responses_series == true_label)).sum()
    TN = ((true_labels_series != true_label) & (participant_responses_series != true_label)).sum()
    FN = ((true_labels_series == true_label) & (participant_responses_series != true_label)).sum()

    # Calculate accuracy for this email
    accuracy = (TP + TN) / len(true_labels_series)  # Total correct predictions / total samples
    print(accuracy)
    # Add the accuracy to the list
    accuracy_list.append(accuracy)

# Calculate the average accuracy across all 10 emails
average_accuracy = sum(accuracy_list) / len(accuracy_list)

# Display the average accuracy
print(f'Average Accuracy for All 10 Emails: {average_accuracy * 100:.2f}%')

import pandas as pd

sheet_data = sheet2

# True labels for the 10 emails (based on your given data)
true_labels_dict = {
    'Email-1': 'Phishing',
    'Email-2': 'Non-phishing',
    'Email-3': 'Non-phishing',
    'Email-4': 'Phishing',
    'Email-5': 'Phishing',
    'Email-6': 'Non-phishing',
    'Email-7': 'Phishing',
    'Email-8': 'Non-phishing',
    'Email-9': 'Phishing',
    'Email-10': 'Non-phishing'
}

# Initialize lists to store accuracy for phishing and non-phishing emails
accuracy_phishing_list = []
accuracy_non_phishing_list = []

# Iterate over all emails and calculate individual accuracy for phishing and non-phishing emails
for i in range(1, 11):
    email_column = f'Email-{i}'
    true_label = true_labels_dict[email_column]

    # Get participant responses for this email column (assumed to be in sheet_data)
    participant_responses = sheet_data[email_column].str.strip().str.capitalize()  # Clean up responses

    # Convert to pandas Series for proper comparison
    true_labels_series = [true_label] * len(participant_responses)
    true_labels_series = pd.Series(true_labels_series)
    participant_responses_series = pd.Series(participant_responses)

    # Calculate True Positive (TP), False Positive (FP), True Negative (TN), False Negative (FN)
    TP = ((true_labels_series == true_label) & (participant_responses_series == true_label)).sum()
    FP = ((true_labels_series != true_label) & (participant_responses_series == true_label)).sum()
    TN = ((true_labels_series != true_label) & (participant_responses_series != true_label)).sum()
    FN = ((true_labels_series == true_label) & (participant_responses_series != true_label)).sum()

    # Calculate accuracy for this email
    accuracy = (TP + TN) / len(true_labels_series)  # Total correct predictions / total samples
    print(accuracy, true_label)
    # If the true label is Phishing, add the accuracy to the phishing list
    if true_label == 'Phishing':
        accuracy_phishing_list.append(accuracy)
    # If the true label is Non-phishing, add the accuracy to the non-phishing list
    elif true_label == 'Non-phishing':
        accuracy_non_phishing_list.append(accuracy)

# Calculate the average accuracy for phishing emails
average_accuracy_phishing = sum(accuracy_phishing_list) / len(accuracy_phishing_list)

# Calculate the average accuracy for non-phishing emails
average_accuracy_non_phishing = sum(accuracy_non_phishing_list) / len(accuracy_non_phishing_list)

# Display the results
print(f'Average Accuracy for Phishing Emails: {average_accuracy_phishing * 100:.2f}%')
print(f'Average Accuracy for Non-Phishing Emails: {average_accuracy_non_phishing * 100:.2f}%')

import matplotlib.pyplot as plt

# Data for the task
labels = ['Phishing Emails', 'Non-Phishing Emails', 'Overall Accuracy']
accuracy_values = [58.18, 54.18, 56.18]  # Corresponding accuracy values

# Create a bar chart
plt.figure(figsize=(8, 6))
bars = plt.bar(labels, accuracy_values, color=['#ff7f0e', '#1f77b4', '#2ca02c'])

# Add values on top of the bars
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.5, f'{yval:.2f}%', ha='center', va='bottom', fontsize=16)

# Add title and labels
plt.title('Task 1', fontsize=14)
plt.xlabel('Category', fontsize=16)
plt.ylabel('Accuracy (%)', fontsize=16)

# Increase the font size of x and y axis data points
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)

# Enable gridlines in the background
plt.grid(True, axis='y', linestyle='--', alpha=0.3)

# Set y-axis limit to scale up to 80%
plt.ylim(0, 80)

# Show the plot
# Save the plot in PDF format with high resolution (300 DPI)
plt.tight_layout()
plt.savefig("accuracy_comparison_task_1.pdf", format='pdf', dpi=300)
plt.show()

"""**For sheet3, I mean Task 2**"""

sheet3.columns

import pandas as pd

# Assuming sheet3 is already loaded, so we'll use it directly.
# For example: sheet3 = pd.read_excel('your_file.xlsx', sheet_name='Sheet3')

# Example: Load the sheet3 DataFrame (you can replace this with loading your specific sheet)
sheet_data = sheet3  # Use sheet3 instead of reading from the file directly

# True labels for the 10 emails (based on your updated data)
true_labels_dict = {
    'Email-1': 'Non-phishing',
    'Email-2': 'Phishing',
    'Email-3': 'Phishing',
    'Email-4': 'Phishing',
    'Email-5': 'Non-phishing',
    'Email-6': 'Non-phishing',
    'Email-7': 'Non-phishing',
    'Email-8': 'Phishing',
    'Email-9': 'Non-phishing',
    'Email-10': 'Phishing'
}

# Iterate over all emails and calculate performance metrics
for i in range(1, 11):
    # Define the current email column and true label
    email_column = f'Email-{i}'
    true_label = true_labels_dict[email_column]

    # Get participant responses for this email column (assumed to be in sheet_data)
    participant_responses = sheet_data[email_column].str.strip().str.capitalize()  # Clean up responses

    # Convert to pandas Series for proper comparison
    true_labels_series = [true_label] * len(participant_responses)
    true_labels_series = pd.Series(true_labels_series)
    participant_responses_series = pd.Series(participant_responses)

    # Calculate True Positive (TP), False Positive (FP), True Negative (TN), False Negative (FN)
    TP = ((true_labels_series == true_label) & (participant_responses_series == true_label)).sum()
    FP = ((true_labels_series != true_label) & (participant_responses_series == true_label)).sum()
    TN = ((true_labels_series != true_label) & (participant_responses_series != true_label)).sum()
    FN = ((true_labels_series == true_label) & (participant_responses_series != true_label)).sum()

    # Calculate Accuracy, Precision, Recall, and F1 Score
    accuracy = (TP + TN) / len(true_labels_series)  # Total correct predictions / total samples
    precision = TP / (TP + FP) if (TP + FP) != 0 else 0  # Avoid division by zero
    recall = TP / (TP + FN) if (TP + FN) != 0 else 0  # Avoid division by zero
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0  # Avoid division by zero

    # Display the results for each email
    print(f'Results for {email_column}:')
    print(f'  Accuracy: {accuracy * 100:.2f}%')
    print(f'  Precision: {precision * 100:.2f}%')
    print(f'  Recall: {recall * 100:.2f}%')
    print(f'  F1 Score: {f1_score * 100:.2f}%')
    print('---')

import pandas as pd

# Assuming you have already loaded the Excel data as:
# sheet1, sheet2, sheet3, sheet4, sheet5
sheet_data = sheet3

# True labels for the 10 emails (based on your new data)
true_labels_dict = {
    'Email-1': 'Non-phishing',
    'Email-2': 'Phishing',
    'Email-3': 'Phishing',
    'Email-4': 'Phishing',
    'Email-5': 'Non-phishing',
    'Email-6': 'Non-phishing',
    'Email-7': 'Non-phishing',
    'Email-8': 'Phishing',
    'Email-9': 'Non-phishing',
    'Email-10': 'Phishing'
}

# Initialize a list to store the accuracy for each email
accuracy_list = []

# Iterate over all emails and calculate individual accuracy
for i in range(1, 11):
    email_column = f'Email-{i}'
    true_label = true_labels_dict[email_column]

    # Get participant responses for this email column (assumed to be in sheet_data)
    participant_responses = sheet_data[email_column].str.strip().str.capitalize()  # Clean up responses

    # Convert to pandas Series for proper comparison
    true_labels_series = [true_label] * len(participant_responses)
    true_labels_series = pd.Series(true_labels_series)
    participant_responses_series = pd.Series(participant_responses)

    # Calculate True Positive (TP), False Positive (FP), True Negative (TN), False Negative (FN)
    TP = ((true_labels_series == true_label) & (participant_responses_series == true_label)).sum()
    FP = ((true_labels_series != true_label) & (participant_responses_series == true_label)).sum()
    TN = ((true_labels_series != true_label) & (participant_responses_series != true_label)).sum()
    FN = ((true_labels_series == true_label) & (participant_responses_series != true_label)).sum()

    # Calculate accuracy for this email
    accuracy = (TP + TN) / len(true_labels_series)  # Total correct predictions / total samples
    print(accuracy)
    # Add the accuracy to the list
    accuracy_list.append(accuracy)

# Calculate the average accuracy across all 10 emails
average_accuracy = sum(accuracy_list) / len(accuracy_list)

# Display the average accuracy
print(f'Average Accuracy for All 10 Emails: {average_accuracy * 100:.2f}%')

import pandas as pd

# Assuming sheet_data is already loaded (e.g., sheet3)
sheet_data = sheet3

# True labels for the 10 emails (based on your updated data)
true_labels_dict = {
    'Email-1': 'Non-phishing',
    'Email-2': 'Phishing',
    'Email-3': 'Phishing',
    'Email-4': 'Phishing',
    'Email-5': 'Non-phishing',
    'Email-6': 'Non-phishing',
    'Email-7': 'Non-phishing',
    'Email-8': 'Phishing',
    'Email-9': 'Non-phishing',
    'Email-10': 'Phishing'
}

# Initialize lists to store accuracy for phishing and non-phishing emails
accuracy_phishing_list = []
accuracy_non_phishing_list = []

# Iterate over all emails and calculate individual accuracy for phishing and non-phishing emails
for i in range(1, 11):
    email_column = f'Email-{i}'
    true_label = true_labels_dict[email_column]

    # Get participant responses for this email column (assumed to be in sheet_data)
    participant_responses = sheet_data[email_column].str.strip().str.capitalize()  # Clean up responses

    # Convert to pandas Series for proper comparison
    true_labels_series = [true_label] * len(participant_responses)
    true_labels_series = pd.Series(true_labels_series)
    participant_responses_series = pd.Series(participant_responses)

    # Calculate True Positive (TP), False Positive (FP), True Negative (TN), False Negative (FN)
    TP = ((true_labels_series == true_label) & (participant_responses_series == true_label)).sum()
    FP = ((true_labels_series != true_label) & (participant_responses_series == true_label)).sum()
    TN = ((true_labels_series != true_label) & (participant_responses_series != true_label)).sum()
    FN = ((true_labels_series == true_label) & (participant_responses_series != true_label)).sum()

    # Calculate accuracy for this email
    accuracy = (TP + TN) / len(true_labels_series)  # Total correct predictions / total samples
    print(accuracy, true_label)
    # If the true label is Phishing, add the accuracy to the phishing list
    if true_label == 'Phishing':
        accuracy_phishing_list.append(accuracy)
    # If the true label is Non-phishing, add the accuracy to the non-phishing list
    elif true_label == 'Non-phishing':
        accuracy_non_phishing_list.append(accuracy)

# Calculate the average accuracy for phishing emails
average_accuracy_phishing = sum(accuracy_phishing_list) / len(accuracy_phishing_list)

# Calculate the average accuracy for non-phishing emails
average_accuracy_non_phishing = sum(accuracy_non_phishing_list) / len(accuracy_non_phishing_list)

# Display the results
print(f'Average Accuracy for Phishing Emails: {average_accuracy_phishing * 100:.2f}%')
print(f'Average Accuracy for Non-Phishing Emails: {average_accuracy_non_phishing * 100:.2f}%')

import matplotlib.pyplot as plt

# Data for Task 2
labels_task2 = ['Phishing Emails', 'Non-Phishing Emails', 'Overall Accuracy']
accuracy_values_task2 = [68.73, 45.45, 57.09]  # Corresponding accuracy values for Task 2

# Create a bar chart for Task 2
plt.figure(figsize=(8, 6))
bars_task2 = plt.bar(labels_task2, accuracy_values_task2, color=['#ff7f0e', '#1f77b4', '#2ca02c'])

# Add values on top of the bars
for bar in bars_task2:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.5, f'{yval:.2f}%', ha='center', va='bottom', fontsize=16)

# Add title and labels
plt.title('Task 2', fontsize=14)
plt.xlabel('Category', fontsize=16)
plt.ylabel('Accuracy (%)', fontsize=16)

# Increase the font size of x and y axis data points
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)

# Enable gridlines in the background
plt.grid(True, axis='y', linestyle='--', alpha=0.3)

# Set y-axis limit to scale up to 80%
plt.ylim(0, 80)

# Show the plot
plt.tight_layout()
plt.savefig("accuracy_comparison_task_2.pdf", format='pdf', dpi=300)
plt.show()

"""**For sheet5**"""

sheet5.columns

import pandas as pd

# Assuming sheet_data is already loaded (e.g., sheet5 in your case)
sheet_data = sheet5  # Adjust to the sheet you're working with

# True labels for the 10 emails (based on your updated data)
true_labels_dict = {
    'Email-1': 'Non-phishing',
    'Email-2': 'Non-phishing',
    'Email-3': 'Phishing',
    'Email-4': 'Non-phishing',
    'Email-5': 'Phishing',
    'Email-6': 'Phishing',
    'Email-7': 'Non-phishing',
    'Email-8': 'Phishing',
    'Email-9': 'Phishing',
    'Email-10': 'Non-phishing'
}

# Initialize a list to store the accuracy for each email
accuracy_list = []

# Iterate over all emails and calculate individual accuracy
for i in range(1, 11):
    email_column = f'Email-{i}'
    true_label = true_labels_dict[email_column]

    # Get participant responses for this email column (assumed to be in sheet_data)
    participant_responses = sheet_data[email_column].str.strip().str.capitalize()  # Clean up responses

    # Convert to pandas Series for proper comparison
    true_labels_series = [true_label] * len(participant_responses)
    true_labels_series = pd.Series(true_labels_series)
    participant_responses_series = pd.Series(participant_responses)

    # Calculate True Positive (TP), False Positive (FP), True Negative (TN), False Negative (FN)
    TP = ((true_labels_series == true_label) & (participant_responses_series == true_label)).sum()
    FP = ((true_labels_series != true_label) & (participant_responses_series == true_label)).sum()
    TN = ((true_labels_series != true_label) & (participant_responses_series != true_label)).sum()
    FN = ((true_labels_series == true_label) & (participant_responses_series != true_label)).sum()

    # Calculate accuracy for this email
    accuracy = (TP + TN) / len(true_labels_series)  # Total correct predictions / total samples
    print(accuracy)
    # Add the accuracy to the list
    accuracy_list.append(accuracy)

# Calculate the average accuracy across all 10 emails
average_accuracy = sum(accuracy_list) / len(accuracy_list)

# Display the average accuracy
print(f'Average Accuracy for All 10 Emails: {average_accuracy * 100:.2f}%')

import pandas as pd

# Assuming sheet_data is already loaded (e.g., sheet5 in your case)
sheet_data = sheet5  # Adjust to the sheet you're working with

# True labels for the 10 emails (based on your updated data)
true_labels_dict = {
    'Email-1': 'Non-phishing',
    'Email-2': 'Non-phishing',
    'Email-3': 'Phishing',
    'Email-4': 'Non-phishing',
    'Email-5': 'Phishing',
    'Email-6': 'Phishing',
    'Email-7': 'Non-phishing',
    'Email-8': 'Phishing',
    'Email-9': 'Phishing',
    'Email-10': 'Non-phishing'
}

# Initialize lists to store accuracy for phishing and non-phishing emails
accuracy_phishing_list = []
accuracy_non_phishing_list = []

# Iterate over all emails and calculate individual accuracy for phishing and non-phishing emails
for i in range(1, 11):
    email_column = f'Email-{i}'
    true_label = true_labels_dict[email_column]

    # Get participant responses for this email column (assumed to be in sheet_data)
    participant_responses = sheet_data[email_column].str.strip().str.capitalize()  # Clean up responses

    # Convert to pandas Series for proper comparison
    true_labels_series = [true_label] * len(participant_responses)
    true_labels_series = pd.Series(true_labels_series)
    participant_responses_series = pd.Series(participant_responses)

    # Calculate True Positive (TP), False Positive (FP), True Negative (TN), False Negative (FN)
    TP = ((true_labels_series == true_label) & (participant_responses_series == true_label)).sum()
    FP = ((true_labels_series != true_label) & (participant_responses_series == true_label)).sum()
    TN = ((true_labels_series != true_label) & (participant_responses_series != true_label)).sum()
    FN = ((true_labels_series == true_label) & (participant_responses_series != true_label)).sum()

    # Calculate accuracy for this email
    accuracy = (TP + TN) / len(true_labels_series)  # Total correct predictions / total samples
    print(accuracy, true_label)
    # If the true label is Phishing, add the accuracy to the phishing list
    if true_label == 'Phishing':
        accuracy_phishing_list.append(accuracy)
    # If the true label is Non-phishing, add the accuracy to the non-phishing list
    elif true_label == 'Non-phishing':
        accuracy_non_phishing_list.append(accuracy)

# Calculate the average accuracy for phishing emails
average_accuracy_phishing = sum(accuracy_phishing_list) / len(accuracy_phishing_list)

# Calculate the average accuracy for non-phishing emails
average_accuracy_non_phishing = sum(accuracy_non_phishing_list) / len(accuracy_non_phishing_list)

# Display the results
print(f'Average Accuracy for Phishing Emails: {average_accuracy_phishing * 100:.2f}%')
print(f'Average Accuracy for Non-Phishing Emails: {average_accuracy_non_phishing * 100:.2f}%')

import matplotlib.pyplot as plt

# Data for Task 3
labels_task3 = ['Phishing Emails', 'Non-Phishing Emails', 'Overall Accuracy']
accuracy_values_task3 = [75.27, 64.73, 70.00]  # Corresponding accuracy values for Task 3

# Create a bar chart for Task 3
plt.figure(figsize=(8, 6))
bars_task3 = plt.bar(labels_task3, accuracy_values_task3, color=['#ff7f0e', '#1f77b4', '#2ca02c'])

# Add values on top of the bars
for bar in bars_task3:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.5, f'{yval:.2f}%', ha='center', va='bottom', fontsize=16)

# Add title and labels
plt.title('Task 3', fontsize=14)
plt.xlabel('Category', fontsize=16)
plt.ylabel('Accuracy (%)', fontsize=16)

# Increase the font size of x and y axis data points
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)

# Enable gridlines in the background
plt.grid(True, axis='y', linestyle='--', alpha=0.3)

# Set y-axis limit to scale up to 80%
plt.ylim(0, 80)

# Show the plot
plt.tight_layout()
plt.savefig("accuracy_comparison_task_3.pdf", format='pdf', dpi=300)
plt.show()

import pandas as pd
from scipy.stats import pearsonr

# Example data: Replace this with your actual data
data = {
    'releases_per_year': [10, 20, 30, 40, 50],  # Example x values
    'avg_outdated_time_per_release_days': [100, 90, 80, 70, 60]  # Example y values
}

df = pd.DataFrame(data)

# Pearson correlation using pandas
correlation = df['releases_per_year'].corr(df['avg_outdated_time_per_release_days'])

# Pearson correlation using scipy
correlation_scipy, _ = pearsonr(df['releases_per_year'], df['avg_outdated_time_per_release_days'])

print(f"Pearson Correlation using pandas: {correlation}")
print(f"Pearson Correlation using scipy: {correlation_scipy}")

import matplotlib.pyplot as plt

plt.scatter(df['releases_per_year'], df['avg_outdated_time_per_release_days'])
plt.title('Correlation between Releases per Year and Avg Outdated Time per Release')
plt.xlabel('Releases per Year')
plt.ylabel('Avg Outdated Time per Release (days)')
plt.show()

import matplotlib.pyplot as plt

# Data for the tasks and completion times
tasks = ['Task-1', 'Task-2', 'Task-3']
completion_times = [14.11, 6.96, 8.29]  # Average completion times in minutes

# Lighter, similar colors
light_colors = ['#FFECB3', '#B3E5FC', '#C8E6C9']

# Create a bar chart
plt.figure(figsize=(6, 3.7))
bars = plt.bar(tasks, completion_times, color=light_colors, width=0.7)  # Adjust the width for narrower bars

# Add values on top of the bars
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.2, f'{yval:.2f}', ha='center', va='bottom', fontsize=20)

plt.xticks(fontsize=20)
plt.yticks(fontsize=20)

# Add title and labels
plt.ylabel('Time (in minutes)', fontsize=22)

plt.grid(True, axis='y', linestyle='--', alpha=0.3)

plt.ylim(0, 16)
# Show the plot
plt.tight_layout()
plt.savefig("completion_times.pdf", format='pdf', dpi=300)
plt.show()

import matplotlib.pyplot as plt

# Data for the tasks and completion times
#tasks = ['Task-1', 'Task-2', 'Task-3']
#completion_times = [14.11, 6.96, 8.29]  # Average completion times in minutes

tasks = ['Task-3', 'Task-2', 'Task-1']
completion_times = [8.29, 6.96, 14.11]
# Lighter, similar colors
light_colors = ['#E57373', '#50C878', '#A2DAE0']

# Create a horizontal bar chart
plt.figure(figsize=(5, 2))
bars = plt.barh(tasks, completion_times, color=light_colors, height=0.5)  # Adjust the height for thinner bars

# Add values on the bars
for bar in bars:
    xval = bar.get_width()
    plt.text(xval + 0.5, bar.get_y() + bar.get_height()/2, f'{xval:.2f}', ha='left', va='center', fontsize=12)

plt.xticks(fontsize=14)
plt.yticks(fontsize=14)

# Add title and labels
plt.xlabel('Time (in minutes)', fontsize=16)

plt.grid(True, axis='x', linestyle='--', alpha=0.3)

plt.xlim(0, 17)
# Show the plot
plt.tight_layout()
plt.savefig("completion_times_horizontal.pdf", format='pdf', dpi=300)
plt.show()

import matplotlib.pyplot as plt

# Data for the tasks and completion times
tasks = ['Task-3', 'Task-2', 'Task-1']
completion_times = [8.29, 6.96, 14.11]

# Define colors and hatch patterns
#light_colors = ['#CD5C5C', '#48AB60', '#1520A6']
light_colors = ['#FFABAB', '#50C878', '#A2DAE0']
#hatch_patterns = ['x', '//', '.']  # Different hatch patterns

# Create a horizontal bar chart
plt.figure(figsize=(5, 2))
bars = plt.barh(tasks, completion_times, color=light_colors, height=0.5, edgecolor='black')

# Apply hatch patterns to bars
for bar, pattern in zip(bars, hatch_patterns):
    bar.set_hatch(pattern)

# Add values on the bars
for bar in bars:
    xval = bar.get_width()
    plt.text(xval + 0.5, bar.get_y() + bar.get_height()/2, f'{xval:.2f}', ha='left', va='center', fontsize=12)

plt.xticks(fontsize=14)
plt.yticks(fontsize=14)

# Add title and labels
plt.xlabel('Time (in minutes)', fontsize=16)

plt.grid(True, axis='x', linestyle='--', alpha=0.3)

plt.xlim(0, 17)

# Show the plot
plt.tight_layout()
plt.savefig("completion_times_horizontal_patterned.pdf", format='pdf', dpi=300)
plt.show()

import matplotlib.pyplot as plt

# Data for the tasks and completion times
tasks = ['Task-1', 'Task-2', 'Task-3']
completion_times = [14.11, 6.96, 8.29]  # Average completion times in minutes

# Lighter, similar colors
light_colors = ['#FFECB3', '#B3E5FC', '#C8E6C9']

#light_colors = ['#FFB6C1', '#98FB98', '#ADD8E6']  # Soft shades of blue/green
#light_colors = ['#B0B0B0', '#D3D3D3', '#A9A9A9']
# Create a bar chart
plt.figure(figsize=(8, 6))
bars = plt.bar(tasks, completion_times, color=light_colors)

# Add values on top of the bars
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.2, f'{yval:.2f}', ha='center', va='bottom', fontsize=16)


plt.xticks(fontsize=14)
plt.yticks(fontsize=14)

# Add title and labels
#plt.title('Average Task Completion Times', fontsize=14)
#plt.xlabel('Task', fontsize=16)
plt.ylabel('Time (in minutes)', fontsize=16)

plt.grid(True, axis='y', linestyle='--', alpha=0.3)

plt.ylim(0, 16)
# Show the plot
plt.tight_layout()
plt.savefig("completion_times.pdf", format='pdf', dpi=300)
plt.show()

"""# Statistical Analysis"""

import pandas as pd

# Load the Excel file
file_path = '/path/to//Combined_ready.xlsx'  # replace with your file name
excel_data = pd.ExcelFile(file_path)

# List all sheet names to see the available sheets
excel_data.sheet_names

# Load specific sheets into separate DataFrames
sheet1 = pd.read_excel(excel_data, sheet_name='Form Responses 1')
sheet2 = pd.read_excel(excel_data, sheet_name='Form Responses 2')
sheet3 = pd.read_excel(excel_data, sheet_name='Form Responses 3')
sheet4 = pd.read_excel(excel_data, sheet_name='Form Responses 4')
sheet5 = pd.read_excel(excel_data, sheet_name='Form Responses 5')

import pandas as pd


# Example: Load the sheet2 DataFrame from an Excel file
sheet_data = sheet2

# True labels for the 10 emails (based on your given data)
true_labels_dict = {
    'Email-1': 'Phishing',
    'Email-2': 'Non-phishing',
    'Email-3': 'Non-phishing',
    'Email-4': 'Phishing',
    'Email-5': 'Phishing',
    'Email-6': 'Non-phishing',
    'Email-7': 'Phishing',
    'Email-8': 'Non-phishing',
    'Email-9': 'Phishing',
    'Email-10': 'Non-phishing'
}

# Initialize a list to store each participant's task 1 accuracy
participant_accuracies = []

# Iterate over participants to calculate their detection accuracy
for index, row in sheet_data.iterrows():
    correct_predictions = 0
    total_emails = len(true_labels_dict)

    for email, true_label in true_labels_dict.items():
        participant_response = row[email].strip().capitalize()  # Clean up response
        if participant_response == true_label:
            correct_predictions += 1

    # Calculate accuracy for the participant
    accuracy = correct_predictions / total_emails
    participant_accuracies.append(accuracy)

# Create a new DataFrame with participants' accuracies
daf = sheet_data[['Email Address']].copy()
daf['task1_accuracy'] = participant_accuracies

daf.head()

import pandas as pd


# Example: Load the sheet2 DataFrame from an Excel file
sheet_data = sheet3

# True labels for the 10 emails (based on your given data)
true_labels_dict = {
    'Email-1': 'Non-phishing',
    'Email-2': 'Phishing',
    'Email-3': 'Phishing',
    'Email-4': 'Phishing',
    'Email-5': 'Non-phishing',
    'Email-6': 'Non-phishing',
    'Email-7': 'Non-phishing',
    'Email-8': 'Phishing',
    'Email-9': 'Non-phishing',
    'Email-10': 'Phishing'
}

# Initialize a list to store each participant's task 1 accuracy
participant_accuracies = []

# Iterate over participants to calculate their detection accuracy
for index, row in sheet_data.iterrows():
    correct_predictions = 0
    total_emails = len(true_labels_dict)

    for email, true_label in true_labels_dict.items():
        participant_response = row[email].strip().capitalize()  # Clean up response
        if participant_response == true_label:
            correct_predictions += 1

    # Calculate accuracy for the participant
    accuracy = correct_predictions / total_emails
    participant_accuracies.append(accuracy)

# Create a new DataFrame with participants' accuracies
#daf = sheet_data[['Email Address']].copy()
daf['task2_accuracy'] = participant_accuracies

daf.head()

import pandas as pd


# Example: Load the sheet2 DataFrame from an Excel file
sheet_data = sheet5

# True labels for the 10 emails (based on your given data)
true_labels_dict = {
    'Email-1': 'Non-phishing',
    'Email-2': 'Non-phishing',
    'Email-3': 'Phishing',
    'Email-4': 'Non-phishing',
    'Email-5': 'Phishing',
    'Email-6': 'Phishing',
    'Email-7': 'Non-phishing',
    'Email-8': 'Phishing',
    'Email-9': 'Phishing',
    'Email-10': 'Non-phishing'
}

# Initialize a list to store each participant's task 1 accuracy
participant_accuracies = []

# Iterate over participants to calculate their detection accuracy
for index, row in sheet_data.iterrows():
    correct_predictions = 0
    total_emails = len(true_labels_dict)

    for email, true_label in true_labels_dict.items():
        participant_response = row[email].strip().capitalize()  # Clean up response
        if participant_response == true_label:
            correct_predictions += 1

    # Calculate accuracy for the participant
    accuracy = correct_predictions / total_emails
    participant_accuracies.append(accuracy)

# Create a new DataFrame with participants' accuracies
#daf = sheet_data[['Email Address']].copy()
daf['task3_accuracy'] = participant_accuracies

daf.head()

daf['task1_accuracy'] = daf['task1_accuracy'] * 100
daf['task2_accuracy'] = daf['task2_accuracy'] * 100
daf['task3_accuracy'] = daf['task3_accuracy'] * 100

daf.head()

import matplotlib.pyplot as plt

# Plot histograms for each task
for task in ['task1_accuracy', 'task2_accuracy', 'task3_accuracy']:
    plt.hist(daf[task], bins=10, alpha=0.7, label=task)
    plt.title(f'Histogram of {task}')
    plt.xlabel('Accuracy (%)')
    plt.ylabel('Frequency')
    plt.show()

from scipy.stats import shapiro

# Apply Shapiro-Wilk test for each task
for task in ['task1_accuracy', 'task2_accuracy', 'task3_accuracy']:
    stat, p = shapiro(daf[task])
    print(f'{task} - Shapiro-Wilk Test: Statistic={stat:.3f}, p={p:.3f}')

"""Interpretation of Normality Tests

The results of the normality tests for each task indicate the following:

Shapiro-Wilk Test Results:

Task 1 Accuracy:
p
=
0.001
p=0.001 (statistic = 0.913)

Task 2 Accuracy:
p
=
0.006
p=0.006 (statistic = 0.936)

Task 3 Accuracy:
p
=
0.005
p=0.005 (statistic = 0.935)

Since all
p
p-values are less than 0.05, the null hypothesis of normality is rejected for all tasks. This suggests that the data for all three tasks is not normally distributed.
"""

from scipy.stats import kstest

# Apply Kolmogorov-Smirnov test for each task
for task in ['task1_accuracy', 'task2_accuracy', 'task3_accuracy']:
    stat, p = kstest(daf[task], 'norm', args=(daf[task].mean(), daf[task].std()))
    print(f'{task} - Kolmogorov-Smirnov Test: Statistic={stat:.3f}, p={p:.3f}')

"""Kolmogorov-Smirnov Test Results:

Task 1 Accuracy:
p
=
0.026
p=0.026 (statistic = 0.195)

Task 2 Accuracy:
p
=
0.056
p=0.056 (statistic = 0.177)

Task 3 Accuracy:
p
=
0.361
p=0.361 (statistic = 0.122)

For Task 1 Accuracy,
p
=
0.026
p=0.026, which is less than 0.05. This indicates the data is not normally distributed.

For Task 2 Accuracy,
p
=
0.056
p=0.056, which is slightly greater than 0.05, suggesting marginal evidence for normality.

For Task 3 Accuracy,
p
=
0.361
p=0.361, which is much greater than 0.05, indicating that the data is likely normally distributed.

**Overall Conclusion**

The Shapiro-Wilk test, being more sensitive to small deviations, suggests that all three tasks' accuracy data are not normally distributed.

The Kolmogorov-Smirnov test offers mixed results, suggesting that Task 2 and Task 3 might be closer to normality.

Given the more consistent rejection of normality by the Shapiro-Wilk test and marginal evidence for Task 2, it is safer to assume that the data is not normally distributed for statistical analysis.
"""

from scipy.stats import friedmanchisquare

# Extract accuracy data for each task
task1 = daf['task1_accuracy']
task2 = daf['task2_accuracy']
task3 = daf['task3_accuracy']

# Apply Friedman test
stat, p = friedmanchisquare(task1, task2, task3)
print(f'Friedman Test: Statistic={stat:.3f}, p={p:.3f}')

# Interpret the result
if p < 0.05:
    print("Significant differences exist among tasks.")
else:
    print("No significant differences among tasks.")

from scipy.stats import wilcoxon

# Pairwise comparisons
pairs = [('task1_accuracy', 'task2_accuracy'),
         ('task1_accuracy', 'task3_accuracy'),
         ('task2_accuracy', 'task3_accuracy')]

for pair in pairs:
    task_a, task_b = pair
    stat, p = wilcoxon(daf[task_a], daf[task_b])
    print(f'Wilcoxon Test {task_a} vs {task_b}: Statistic={stat:.3f}, p={p:.3f}')

    # Interpret the result
    if p < 0.05:
        print(f"Significant difference between {task_a} and {task_b}.")
    else:
        print(f"No significant difference between {task_a} and {task_b}.")

"""Interpretation of Results

Friedman Test:
p
<
0.05
p<0.05: There is a significant difference in accuracies across the tasks.
p
≥
0.05
p≥0.05: No significant difference across tasks.

Wilcoxon Signed-Rank Test:
p
<
0.05
p<0.05: There is a significant difference between the two tasks being compared.
p
≥
0.05
p≥0.05: No significant difference between the two tasks.
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'daf' is your DataFrame
melted_data = daf.melt(id_vars='Email Address', value_vars=['task1_accuracy', 'task2_accuracy', 'task3_accuracy'],
                       var_name='Task', value_name='Accuracy')

# Create a boxplot for task accuracies
plt.figure(figsize=(8, 6))
sns.boxplot(x='Task', y='Accuracy', data=melted_data, showmeans=False, medianprops={'visible': False})

# Highlight the mean of each task with a horizontal line
means = melted_data.groupby('Task')['Accuracy'].mean()
for i, task in enumerate(means.index):
    plt.plot([i - 0.4, i + 0.4], [means[task], means[task]], color='black', lw=1, zorder=3)  # Horizontal line at the mean

# Customize the plot
#plt.title('Task Accuracy Comparison')
plt.ylabel('Accuracy (%)')
plt.xlabel('Task')
plt.ylim(0, 100)  # Ensure the y-axis is scaled appropriately for accuracy

plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'daf' is your DataFrame
melted_data = daf.melt(id_vars='Email Address', value_vars=['task1_accuracy', 'task2_accuracy', 'task3_accuracy'],
                       var_name='Task', value_name='Accuracy')

# Create a boxplot for task accuracies
plt.figure(figsize=(8, 6))
sns.boxplot(x='Task', y='Accuracy', data=melted_data, showmeans=False, medianprops={'visible': False})

# Highlight the mean of each task with a horizontal line
means = melted_data.groupby('Task')['Accuracy'].mean()
for i, task in enumerate(means.index):
    plt.plot([i - 0.4, i + 0.4], [means[task], means[task]], color='black', lw=1, zorder=3)  # Horizontal line at the mean

# Customize the plot
plt.ylabel('Accuracy (%)', fontsize=16)
plt.xlabel('Task', fontsize=16)

plt.ylim(0, 105)

# Change x-axis labels
plt.xticks([0, 1, 2], ['Task 1', 'Task 2', 'Task 3'], fontsize=14)
plt.yticks(fontsize=14)

plt.savefig("participants_accuracy.pdf", format='pdf', dpi=300)
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'daf' is your DataFrame
melted_data = daf.melt(id_vars='Email Address', value_vars=['task1_accuracy', 'task2_accuracy', 'task3_accuracy'],
                       var_name='Task', value_name='Accuracy')

# Define light colors for Task 1, Task 2, and Task 3
light_colors = ['#FFECB3', '#B3E5FC', '#C8E6C9']

# Create a boxplot for task accuracies with custom colors, mean and median
plt.figure(figsize=(8, 6))
sns.boxplot(x='Task', y='Accuracy', data=melted_data, showmeans=True, medianprops={'color': 'red'},
            hue='Task', palette=light_colors, legend=False)

# Calculate the median for each task and display it on the plot
medians = melted_data.groupby('Task')['Accuracy'].median()
for i, task in enumerate(medians.index):
    plt.text(i, medians[task] + 0.2, f'{medians[task]:.2f}', ha='center', va='bottom', fontsize=12, color='black')

# Customize the plot
plt.ylabel('Accuracy (%)', fontsize=16)
plt.xlabel('Task', fontsize=16)
plt.ylim(0, 105)  # Ensure the y-axis is scaled appropriately for accuracy

plt.grid(True, axis='y', linestyle='--', alpha=0.3)

# Change x-axis labels
plt.xticks([0, 1, 2], ['Task 1', 'Task 2', 'Task 3'], fontsize=14)
plt.yticks(fontsize=14)
plt.savefig("participants_accuracy1.pdf", format='pdf', dpi=300)
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'daf' is your DataFrame
melted_data = daf.melt(id_vars='Email Address', value_vars=['task1_accuracy', 'task2_accuracy', 'task3_accuracy'],
                       var_name='Task', value_name='Accuracy')

# Define light colors for Task 1, Task 2, and Task 3
light_colors = ['#FFECB3', '#B3E5FC', '#C8E6C9']

# Create a boxplot for task accuracies with custom colors, mean, and median
plt.figure(figsize=(5, 4))
sns.boxplot(x='Task', y='Accuracy', data=melted_data, showmeans=True, width=0.35,
            medianprops={'color': 'red'}, hue='Task', palette=light_colors, dodge=False)

# Calculate the median for each task and display it on the plot
medians = melted_data.groupby('Task')['Accuracy'].median()
for i, task in enumerate(medians.index):
    plt.text(i, medians[task] + 0.2, f'{medians[task]:.2f}', ha='center', va='bottom', fontsize=11, color='black')

# Customize the plot
plt.ylabel('Accuracy (%)', fontsize=16)
#plt.xlabel('Task', fontsize=16)
plt.ylim(0, 104)  # Ensure the y-axis is scaled appropriately for accuracy
plt.grid(True, axis='y', linestyle='--', alpha=0.3)

# Change x-axis labels
plt.xticks([0, 1, 2], ['Task-1', 'Task-2', 'Task-3'], fontsize=14)
plt.yticks(fontsize=14)

# Add a legend for the boxplot elements
import matplotlib.lines as mlines
mean_marker = mlines.Line2D([], [], color='green', marker='^', linestyle='None', markersize=10, label='Mean')
median_line = mlines.Line2D([], [], color='red', linestyle='-', linewidth=2, label='Median')
plt.legend(handles=[mean_marker, median_line], loc='upper left', fontsize=12, title_fontsize=14)

# Save and display the plot
plt.savefig("PA1C.pdf", format='pdf', dpi=300, bbox_inches='tight')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.lines as mlines

# Assuming 'daf' is your DataFrame
melted_data = daf.melt(id_vars='Email Address', value_vars=['task1_accuracy', 'task2_accuracy', 'task3_accuracy'],
                       var_name='Task', value_name='Accuracy')

# Define light colors for Task 1, Task 2, and Task 3
#light_colors = ['#FFECB3', '#B3E5FC', '#C8E6C9']
light_colors = ['#9897A9', '#FFBF00', '#1520A6']

# Create a boxplot for task accuracies with reduced box width
plt.figure(figsize=(5, 4))
sns.boxplot(x='Task', y='Accuracy', data=melted_data, showmeans=True, width=0.4,  # Reduced box width
            medianprops={'color': 'red'}, hue='Task', palette=light_colors, dodge=False)

# Calculate the median for each task and display it on the plot
medians = melted_data.groupby('Task')['Accuracy'].median()
for i, task in enumerate(medians.index):
    plt.text(i, medians[task] + 0.2, f'{medians[task]:.2f}', ha='center', va='bottom', fontsize=11, color='white')

# Customize the plot
plt.ylabel('Accuracy (%)', fontsize=16)
plt.ylim(0, 104)  # Ensure the y-axis is scaled appropriately for accuracy
plt.grid(True, axis='y', linestyle='--', alpha=0.3)

# Change x-axis labels
plt.xticks([0, 1, 2], ['Task-1', 'Task-2', 'Task-3'], fontsize=14)
plt.yticks(fontsize=14)

# Add a legend for the boxplot elements
mean_marker = mlines.Line2D([], [], color='green', marker='x', linestyle='None', markersize=10, label='Mean')
median_line = mlines.Line2D([], [], color='red', linestyle='--', linewidth=2, label='Median')
plt.legend(handles=[mean_marker, median_line], loc='upper left', fontsize=11, title_fontsize=14)

# Save and display the plot
plt.savefig("PA1C.pdf", format='pdf', dpi=300, bbox_inches='tight')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.lines as mlines
import matplotlib.patches as mpatches

# Assuming 'daf' is your DataFrame
melted_data = daf.melt(id_vars='Email Address', value_vars=['task1_accuracy', 'task2_accuracy', 'task3_accuracy'],
                       var_name='Task', value_name='Accuracy')

# Define light colors for Task 1, Task 2, and Task 3
#light_colors = ['#FFECB3', '#B3E5FC', '#C8E6C9']
light_colors = ['#E57373', '#50C878', '#A2DAE0']
hatch_patterns = ['x', '//', '.']  # Different hatch patterns

# Create a boxplot for task accuracies with reduced box width
plt.figure(figsize=(5, 4))
sns.boxplot(x='Task', y='Accuracy', data=melted_data, showmeans=True, width=0.4,
                                  medianprops={'color': 'red', 'linestyle': '-', 'linewidth': 2},
                      meanprops={'marker': 'x', 'markerfacecolor': 'black', 'markeredgecolor': 'black', 'markersize': 6}, # Reduced box width
            hue='Task', palette=light_colors, dodge=False)

# Calculate the median for each task and display it on the plot
medians = melted_data.groupby('Task')['Accuracy'].median()
for i, task in enumerate(medians.index):
    plt.text(i, medians[task] + 0.2, f'{medians[task]:.2f}', ha='center', va='bottom', fontsize=11, color='black')

# Customize the plot
plt.ylabel('Accuracy (%)', fontsize=16)
plt.ylim(0, 104)  # Ensure the y-axis is scaled appropriately for accuracy
plt.grid(True, axis='y', linestyle='--', alpha=0.3)

# Change x-axis labels
plt.xticks([0, 1, 2], ['Task-1', 'Task-2', 'Task-3'], fontsize=14)
plt.yticks(fontsize=14)

# Add a legend for the boxplot elements
mean_marker = mlines.Line2D([], [], color='black', marker='x', linestyle='None', markersize=8, label='Mean')
median_line = mlines.Line2D([], [], color='red', linestyle='-', linewidth=2, label='Median')
plt.legend(handles=[mean_marker, median_line], loc='upper left', fontsize=11, title_fontsize=14)

# Save and display the plot
plt.savefig("PA1CN.pdf", format='pdf', dpi=300, bbox_inches='tight')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.lines as mlines
import pandas as pd

# Assuming 'daf' is your DataFrame
melted_data = daf.melt(id_vars='Email Address', value_vars=['task1_accuracy', 'task2_accuracy', 'task3_accuracy'],
                       var_name='Task', value_name='Accuracy')

# Reorder the tasks
task_order = ['task1_accuracy', 'task2_accuracy', 'task3_accuracy']
task_labels = {'task1_accuracy': 'Task-1', 'task2_accuracy': 'Task-2', 'task3_accuracy': 'Task-3'}
melted_data['Task'] = pd.Categorical(melted_data['Task'], categories=task_order, ordered=True)

# Define light colors for Task 1, Task 2, and Task 3
light_colors = ['#FFECB3', '#B3E5FC', '#C8E6C9']

# Create a horizontal boxplot for task accuracies with custom colors, mean, and median
plt.figure(figsize=(6, 2))
sns.boxplot(y='Task', x='Accuracy', data=melted_data, showmeans=True,
            medianprops={'color': 'red'}, palette=light_colors, dodge=False, orient='h')

# Calculate the median for each task and display it on the plot
medians = melted_data.groupby('Task')['Accuracy'].median()
for i, (task, label) in enumerate(task_labels.items()):
    plt.text(medians[task] + 2, i, f'{medians[task]:.2f}', ha='left', va='center', fontsize=12, color='black')

# Customize the plot
plt.xlabel('Accuracy (%)', fontsize=14)
plt.ylabel('Task', fontsize=14)
plt.xlim(0, 102)  # Ensure the x-axis is scaled for accuracy
plt.grid(True, axis='x', linestyle='--', alpha=0.3)

# Change y-axis labels
plt.yticks([0, 1, 2], [task_labels[task] for task in task_order], fontsize=14)
plt.xticks(fontsize=12)

# Add a legend within the plot area
mean_marker = mlines.Line2D([], [], color='green', marker='^', linestyle='None', markersize=8, label='Mean')
median_line = mlines.Line2D([], [], color='red', linestyle='-', linewidth=2, label='Median')
plt.legend(handles=[mean_marker, median_line], loc='upper right', frameon=False, fontsize=10)

# Save and display the plot
plt.tight_layout()
plt.savefig("PA1C_horizontal_ordered.pdf", format='pdf', dpi=300)
plt.show()

sheet2.columns

# Rename the column in the DataFrame
sheet2.rename(columns={'How confident do you feel about the response you provided?': 'confidence1'}, inplace=True)
sheet2.rename(columns={'How confident do you feel about the response you provided?.1': 'confidence2'}, inplace=True)
sheet2.rename(columns={'How confident do you feel about the response you provided?.2': 'confidence3'}, inplace=True)
sheet2.rename(columns={'How confident do you feel about the response you provided?.3': 'confidence4'}, inplace=True)
sheet2.rename(columns={'How confident do you feel about the response you provided?.4': 'confidence5'}, inplace=True)
sheet2.rename(columns={'How confident do you feel about the response you provided?.5': 'confidence6'}, inplace=True)
sheet2.rename(columns={'How confident do you feel about the response you provided?.6': 'confidence7'}, inplace=True)
sheet2.rename(columns={'How confident do you feel about the response you provided?.7': 'confidence8'}, inplace=True)
sheet2.rename(columns={'How confident do you feel about the response you provided?.8': 'confidence9'}, inplace=True)
sheet2.rename(columns={'How confident do you feel about the response you provided?.9': 'confidence10'}, inplace=True)

sheet2.head()

# Rename the column in the DataFrame
sheet3.rename(columns={'How confident do you feel about the response you provided?': 'confidence1'}, inplace=True)
sheet3.rename(columns={'How confident do you feel about the response you provided?.1': 'confidence2'}, inplace=True)
sheet3.rename(columns={'How confident do you feel about the response you provided?.2': 'confidence3'}, inplace=True)
sheet3.rename(columns={'How confident do you feel about the response you provided?.3': 'confidence4'}, inplace=True)
sheet3.rename(columns={'How confident do you feel about the response you provided?.4': 'confidence5'}, inplace=True)
sheet3.rename(columns={'How confident do you feel about the response you provided?.5': 'confidence6'}, inplace=True)
sheet3.rename(columns={'How confident do you feel about the response you provided?.6': 'confidence7'}, inplace=True)
sheet3.rename(columns={'How confident do you feel about the response you provided?.7': 'confidence8'}, inplace=True)
sheet3.rename(columns={'How confident do you feel about the response you provided?.8': 'confidence9'}, inplace=True)
sheet3.rename(columns={'How confident do you feel about the response you provided?.9': 'confidence10'}, inplace=True)

sheet3.head()

# Rename the column in the DataFrame
sheet5.rename(columns={'How confident do you feel about the response you provided?': 'confidence1'}, inplace=True)
sheet5.rename(columns={'How confident do you feel about the response you provided?.1': 'confidence2'}, inplace=True)
sheet5.rename(columns={'How confident do you feel about the response you provided?.2': 'confidence3'}, inplace=True)
sheet5.rename(columns={'How confident do you feel about the response you provided?.3': 'confidence4'}, inplace=True)
sheet5.rename(columns={'How confident do you feel about the response you provided?.4': 'confidence5'}, inplace=True)
sheet5.rename(columns={'How confident do you feel about the response you provided?.5': 'confidence6'}, inplace=True)
sheet5.rename(columns={'How confident do you feel about the response you provided?.6': 'confidence7'}, inplace=True)
sheet5.rename(columns={'How confident do you feel about the response you provided?.7': 'confidence8'}, inplace=True)
sheet5.rename(columns={'How confident do you feel about the response you provided?.8': 'confidence9'}, inplace=True)
sheet5.rename(columns={'How confident do you feel about the response you provided?.9': 'confidence10'}, inplace=True)

sheet5.head()

# Adjusting the function to focus only on the confidence columns
def analyze_confidence(task_confidence):
    # Select only columns that contain 'confidence' in their names
    confidence_columns = [col for col in task_confidence.columns if 'confidence' in col.lower()]

    # Flatten all confidence columns into a single series
    confidence_series = task_confidence[confidence_columns].values.flatten()

    # Create a DataFrame for analysis
    confidence_df = pd.DataFrame(confidence_series, columns=["Confidence"])

    # Count occurrences of each confidence level
    distribution = confidence_df["Confidence"].value_counts()

    # Calculate percentages
    percentages = (distribution / len(confidence_series)) * 100

    # Map confidence levels to numerical scores
    mapping = {
        "Not confident at all (0% – 20%)": 1,
        "Slightly confident (21% – 40%)": 2,
        "Moderately confident (41% – 60%)": 3,
        "Confident (61% – 80%)": 4,
        "Very confident (81% – 100%)": 5
    }
    confidence_df["Confidence_Score"] = confidence_df["Confidence"].map(mapping)

    # Compute mean and median confidence levels
    mean_confidence = confidence_df["Confidence_Score"].mean()
    median_confidence = confidence_df["Confidence_Score"].median()

    return distribution, percentages, mean_confidence, median_confidence

# Analyze Task 1
task1_distribution, task1_percentages, task1_mean, task1_median = analyze_confidence(sheet2)

# Analyze Task 2
task2_distribution, task2_percentages, task2_mean, task2_median = analyze_confidence(sheet3)

# Analyze Task 3
task3_distribution, task3_percentages, task3_mean, task3_median = analyze_confidence(sheet5)

# Print results for each task
for task, distribution, percentages, mean, median in zip(
        ["Task 1", "Task 2", "Task 3"],
        [task1_distribution, task2_distribution, task3_distribution],
        [task1_percentages, task2_percentages, task3_percentages],
        [task1_mean, task2_mean, task3_mean],
        [task1_median, task2_median, task3_median]):
    print(f"\n{task} Confidence Analysis:")
    print("Distribution of Confidence Levels:")
    print(distribution)
    print("\nPercentages of Confidence Levels:")
    print(percentages)
    print(f"\nMean Confidence Score: {mean:.2f}")
    print(f"Median Confidence Score: {median:.2f}")



"""based on phishing training, is there any difference in accuracy who has traning, and who hasn't"""

import pandas as pd

# Load the Excel file
file_path = '/path/to/Combined_ready.xlsx'  # replace with your file name
excel_data = pd.ExcelFile(file_path)

# List all sheet names to see the available sheets
excel_data.sheet_names

# Load specific sheets into separate DataFrames
sheet1 = pd.read_excel(excel_data, sheet_name='Form Responses 1')
sheet2 = pd.read_excel(excel_data, sheet_name='Form Responses 2')
sheet3 = pd.read_excel(excel_data, sheet_name='Form Responses 3')
sheet4 = pd.read_excel(excel_data, sheet_name='Form Responses 4')
sheet5 = pd.read_excel(excel_data, sheet_name='Form Responses 5')

sheet1.head(1)

sheet1.rename(columns={'Have you attended any phishing attempt focused training or awareness programs?': 'training'}, inplace=True)

sheet2.head(1)

import pandas as pd

# Assuming sheet_data (sheet2) and sheet1 are already loaded
sheet_data = sheet2

# True labels for the 10 emails (based on your given data)
true_labels_dict = {
    'Email-1': 'Phishing',
    'Email-2': 'Non-phishing',
    'Email-3': 'Non-phishing',
    'Email-4': 'Phishing',
    'Email-5': 'Phishing',
    'Email-6': 'Non-phishing',
    'Email-7': 'Phishing',
    'Email-8': 'Non-phishing',
    'Email-9': 'Phishing',
    'Email-10': 'Non-phishing'
}

# Initialize a list to store each participant's task 1 accuracy
participant_accuracies_task1 = []

# Iterate over participants to calculate their detection accuracy for Task 1
for index, row in sheet_data.iterrows():
    correct_predictions = 0
    total_emails = len(true_labels_dict)

    for email, true_label in true_labels_dict.items():
        participant_response = row[email].strip().capitalize()  # Clean up response
        if participant_response == true_label:
            correct_predictions += 1

    # Calculate accuracy for the participant
    accuracy = correct_predictions / total_emails
    participant_accuracies_task1.append(accuracy)

# Create a new DataFrame with participants' accuracies for Task 1
daf_task1 = sheet_data[['Email Address']].copy()
daf_task1['task1_accuracy'] = participant_accuracies_task1

# Now, merge with sheet1 to add the 'training' column
sheet1_training = sheet1[['Email Address', 'training']].copy()  # Select 'Email Address' and 'training' from sheet1
merged_df_task1 = pd.merge(daf_task1, sheet1_training, on='Email Address', how='left')

# Separate into two DataFrames: one for participants with training ('Yes') and one for those without training ('No')
training_yes_df_task1 = merged_df_task1[merged_df_task1['training'] == 'Yes'][['Email Address', 'task1_accuracy', 'training']]
training_no_df_task1 = merged_df_task1[merged_df_task1['training'] == 'No'][['Email Address', 'task1_accuracy', 'training']]

# Display the DataFrames for those with and without training for Task 1
print("Participants with Training (Yes) for Task 1:")
print(training_yes_df_task1)

print("\nParticipants without Training (No) for Task 1:")
print(training_no_df_task1)

import pandas as pd

# Assuming sheet_data (sheet3) and sheet1 are already loaded
sheet_data = sheet3

# True labels for the 10 emails (based on your given data for Task 2)
true_labels_dict = {
    'Email-1': 'Non-phishing',
    'Email-2': 'Phishing',
    'Email-3': 'Phishing',
    'Email-4': 'Phishing',
    'Email-5': 'Non-phishing',
    'Email-6': 'Non-phishing',
    'Email-7': 'Non-phishing',
    'Email-8': 'Phishing',
    'Email-9': 'Non-phishing',
    'Email-10': 'Phishing'
}

# Initialize a list to store each participant's task 2 accuracy
participant_accuracies_task2 = []

# Iterate over participants to calculate their detection accuracy for Task 2
for index, row in sheet_data.iterrows():
    correct_predictions = 0
    total_emails = len(true_labels_dict)

    for email, true_label in true_labels_dict.items():
        participant_response = row[email].strip().capitalize()  # Clean up response
        if participant_response == true_label:
            correct_predictions += 1

    # Calculate accuracy for the participant
    accuracy = correct_predictions / total_emails
    participant_accuracies_task2.append(accuracy)

# Create a new DataFrame with participants' accuracies for Task 2
daf_task2 = sheet_data[['Email Address']].copy()
daf_task2['task2_accuracy'] = participant_accuracies_task2

# Now, merge with sheet1 to add the 'training' column
sheet1_training = sheet1[['Email Address', 'training']].copy()  # Select 'Email Address' and 'training' from sheet1
merged_df_task2 = pd.merge(daf_task2, sheet1_training, on='Email Address', how='left')

# Separate into two DataFrames: one for participants with training ('Yes') and one for those without training ('No')
training_yes_df_task2 = merged_df_task2[merged_df_task2['training'] == 'Yes'][['Email Address', 'task2_accuracy', 'training']]
training_no_df_task2 = merged_df_task2[merged_df_task2['training'] == 'No'][['Email Address', 'task2_accuracy', 'training']]

# Display the DataFrames for those with and without training for Task 2
print("Participants with Training (Yes) for Task 2:")
print(training_yes_df_task2)

print("\nParticipants without Training (No) for Task 2:")
print(training_no_df_task2)

import pandas as pd

# Assuming sheet_data (sheet5) and sheet1 are already loaded
sheet_data = sheet5

# True labels for the 10 emails (based on your given data for Task 3)
true_labels_dict = {
    'Email-1': 'Non-phishing',
    'Email-2': 'Non-phishing',
    'Email-3': 'Phishing',
    'Email-4': 'Non-phishing',
    'Email-5': 'Phishing',
    'Email-6': 'Phishing',
    'Email-7': 'Non-phishing',
    'Email-8': 'Phishing',
    'Email-9': 'Phishing',
    'Email-10': 'Non-phishing'
}

# Initialize a list to store each participant's task 3 accuracy
participant_accuracies_task3 = []

# Iterate over participants to calculate their detection accuracy for Task 3
for index, row in sheet_data.iterrows():
    correct_predictions = 0
    total_emails = len(true_labels_dict)

    for email, true_label in true_labels_dict.items():
        participant_response = row[email].strip().capitalize()  # Clean up response
        if participant_response == true_label:
            correct_predictions += 1

    # Calculate accuracy for the participant
    accuracy = correct_predictions / total_emails
    participant_accuracies_task3.append(accuracy)

# Create a new DataFrame with participants' accuracies for Task 3
daf_task3 = sheet_data[['Email Address']].copy()
daf_task3['task3_accuracy'] = participant_accuracies_task3

# Now, merge with sheet1 to add the 'training' column
sheet1_training = sheet1[['Email Address', 'training']].copy()  # Select 'Email Address' and 'training' from sheet1
merged_df_task3 = pd.merge(daf_task3, sheet1_training, on='Email Address', how='left')

# Separate into two DataFrames: one for participants with training ('Yes') and one for those without training ('No')
training_yes_df_task3 = merged_df_task3[merged_df_task3['training'] == 'Yes'][['Email Address', 'task3_accuracy', 'training']]
training_no_df_task3 = merged_df_task3[merged_df_task3['training'] == 'No'][['Email Address', 'task3_accuracy', 'training']]

# Display the DataFrames for those with and without training for Task 3
print("Participants with Training (Yes) for Task 3:")
print(training_yes_df_task3)

print("\nParticipants without Training (No) for Task 3:")
print(training_no_df_task3)

import pandas as pd

# Define a function for statistical analysis
def statistical_analysis(df, task_column):
    # Group by training and calculate statistics
    grouped = df.groupby('training')[task_column]

    # Calculate statistics: mean, std, min, max, median
    stats = grouped.describe()

    # Only keep the relevant columns from the describe() output
    stats = stats[['mean', 'std', 'min', '25%', '50%', '75%', 'max']]

    # Rename the columns for better readability
    stats.rename(columns={'mean': 'Average Accuracy',
                          'std': 'Standard Deviation',
                          'min': 'Min Accuracy',
                          '25%': '25th Percentile',
                          '50%': 'Median Accuracy',
                          '75%': '75th Percentile',
                          'max': 'Max Accuracy'}, inplace=True)
    return stats

# Assuming you have the merged DataFrame from the previous steps
# For Task 1:
print("Statistical Analysis for Task 1:")
stats_task1 = statistical_analysis(merged_df_task1, 'task1_accuracy')
print(stats_task1)

print("\n---\n")

# For Task 2:
print("Statistical Analysis for Task 2:")
stats_task2 = statistical_analysis(merged_df_task2, 'task2_accuracy')
print(stats_task2)

print("\n---\n")

# For Task 3:
print("Statistical Analysis for Task 3:")
stats_task3 = statistical_analysis(merged_df_task3, 'task3_accuracy')
print(stats_task3)

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.lines as mlines

# Set up the figure and subplots
fig, axes = plt.subplots(1, 3, figsize=(7, 4.5), sharey=True)  # Share y-axis for consistent scaling
plt.subplots_adjust(wspace=0.05)
fig.tight_layout(pad=2.6)

# Common legend elements
red_line = mlines.Line2D([], [], color='red', linewidth=2, label='Median')
green_triangle = mlines.Line2D([], [], color='black', marker='x', markersize=8, linestyle='None', label='Mean')

# Define colors: "Yes" gets one color, "No" gets another color
yes_color = "#FAEA99"  # Yellow
no_color = "#74B6B2"   # Teal
training_colors = {"Yes": yes_color, "No": no_color}

# Function to create a boxplot for each task
def create_boxplot(ax, data, task_col, ylabel, title):
    # Drop missing values
    data = data.dropna(subset=['training'])

    # Multiply accuracy by 100 to convert it into percentages
    data[task_col] = data[task_col] * 100

    # Create the boxplot with consistent colors for "Yes" and "No"
    sns.boxplot(
        ax=ax,
        x='training',
        y=task_col,
        data=data,
        palette=training_colors,  # Use the predefined color mapping
        width=0.42
    )

    # Calculate and mark the median and mean for each unique training category
    for i, training_label in enumerate(data['training'].unique()):
        median_val = data[data['training'] == training_label][task_col].median()
        mean_val = data[data['training'] == training_label][task_col].mean()

        # Plot the median line
        ax.plot([i - 0.2, i + 0.2], [median_val, median_val], color='red', linewidth=2)
        # Plot the mean marker
        ax.plot([i], [mean_val], color='black', marker='x', markersize=6)

        # Determine text color: White for "No", Black for "Yes"
        text_color = "white" if training_label == "No" else "black"

        # Display the mean value
        ax.text(i, mean_val + 0.6, f'{mean_val:.2f}', color=text_color, ha='center', va='bottom', fontsize=10)

    # Set labels and title
    ax.set_title(title, fontsize=14)
    ax.set_xlabel('' if ylabel == '' else 'Awareness Training', fontsize=12)
    ax.set_ylabel(ylabel, fontsize=12)
    ax.set_xticks(range(len(data['training'].unique())))
    ax.set_xticklabels(data['training'].unique(), fontsize=12)
    ax.set_ylim(0, 102)
    ax.tick_params(axis='y', labelsize=10)

# Plot for Task 1
create_boxplot(
    ax=axes[0],
    data=merged_df_task1,
    task_col='task1_accuracy',
    ylabel='',
    title='Task-1'
)

# Plot for Task 2
create_boxplot(
    ax=axes[1],
    data=merged_df_task2,
    task_col='task2_accuracy',
    ylabel='',
    title='Task-2'
)

# Plot for Task 3
create_boxplot(
    ax=axes[2],
    data=merged_df_task3,
    task_col='task3_accuracy',
    ylabel='',
    title='Task-3'
)

# Add a common x-label for all subplots
fig.text(0.5, 0.02, 'Awareness Training', ha='center', fontsize=14)

# Add a common y-label for all subplots
fig.text(0.02, 0.5, 'Accuracy (%)', va='center', rotation='vertical', fontsize=14)

# Add a shared legend
fig.legend(
    handles=[red_line, green_triangle],
    loc='upper center',
    fontsize=10,
    ncol=2,
    bbox_to_anchor=(0.5, 1.05)  # Adjust the bbox to place the legend on top of the plots
)

# Save the figure as a single PDF
plt.savefig("training_all_tasks_yes_no_colored.pdf", format='pdf', dpi=300, bbox_inches='tight')

# Show the figure
plt.show()